{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation: Segmenation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from src.preprocessing import get_transforms, get_datasets, get_dataloaders\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "# Set the device\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transforms\n",
    "transform = get_transforms()\n",
    "\n",
    "# Get datasets\n",
    "train_ds, val_ds = get_datasets(root_dir=\"../data\", collection=\"HCC-TACE-Seg\", transform=transform, download=False, download_len=1, val_frac=0.2)\n",
    "\n",
    "# Get dataloaders\n",
    "train_loader, val_loader = get_dataloaders(train_ds, val_ds, batch_size=1, num_workers=4)\n",
    "\n",
    "train_dl =train_ds.datalist\n",
    "val_dl = val_ds.datalist\n",
    "\n",
    "\n",
    "# Check length of datasets and dataloaders\n",
    "print(train_ds.get_indices(), val_ds.get_indices())\n",
    "print(len(train_loader), len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a batch of data from the dataloader\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Separate the image and segmentation from the batch\n",
    "image, seg = batch[\"image\"], batch[\"seg\"]\n",
    "\n",
    "print(image.shape, seg.shape)\n",
    "\n",
    "#mip, _ = torch.max(seg, dim=-1)\n",
    "#mip = mip.squeeze()\n",
    "#print(mip.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TACEnet(\n",
       "  (lrm): Conv3DTo2D(\n",
       "    (conv1): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (conv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (conv3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5): Conv2d(256, 1, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       "  (cn): ConvNet(\n",
       "    (conv1): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv3): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model_source.LRM\n",
    "\n",
    "# Create an instance of the model\n",
    "model = model_source.LRM.TACEnet()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VesselVolume = torch.rand(1,96,512,512)\n",
    "DRR = torch.rand(1,1,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(VesselVolume,DRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concatenate((torch.rand(1,1,256,256), torch.rand(1,1,256,256)), dim=1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(mip[2], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Swin-UNet model\n",
    "#model = SwinUNETR(img_size=(96, 96, 32), in_channels=1, out_channels=1, use_v2 = True, spatial_dims=3, normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinUNETRSigmoid(nn.Module):\n",
    "    def __init__(self, img_size, in_channels, out_channels, use_v2=True, spatial_dims=3, normalize=False):\n",
    "        super(SwinUNETRSigmoid, self).__init__()\n",
    "        self.model = SwinUNETR(img_size=img_size, in_channels=in_channels, out_channels=out_channels, use_v2=use_v2, spatial_dims=spatial_dims, normalize=normalize)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Create the model instance\n",
    "model = SwinUNETRSigmoid(img_size=(96, 96, 32), in_channels=1, out_channels=1, use_v2=True, spatial_dims=3, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb_ = True\n",
    "\n",
    "\n",
    "# Set the model to use the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the hyperparameters for training\n",
    "max_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "val_interval = 1\n",
    "\n",
    "if wandb_:\n",
    "    run = wandb.init(\n",
    "        # Set the project where this run will be logged\n",
    "        project=\"HCC TACE\", name=\"test\",\n",
    "        # Track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": {learning_rate},\n",
    "            \"epochs\": {max_epochs},\n",
    "            \"Weight decay\": {weight_decay},\n",
    "            \"Batch_size\": {1},\n",
    "        },\n",
    "    )\n",
    "\n",
    "# Set the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create the training loop\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Iterate over the training data for the specified number of epochs\n",
    "for epoch in range(max_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Extract the image and segmentation from the batch\n",
    "        image, seg = batch[\"image\"].to(device), batch[\"seg\"].to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(image).squeeze()\n",
    "        \n",
    "        print(pred.size())\n",
    "        print(seg.size())\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(pred, seg)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if wandb_:\n",
    "            run.log({\"Train loss\": loss.item()})\n",
    "        \n",
    "    # Print the loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{max_epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Compute the validation loss\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            image, seg = batch[\"image\"].to(device), batch[\"seg\"].to(device)\n",
    "            pred = model(image).squeeze()\n",
    "            \n",
    "            loss = loss_function(pred, seg)\n",
    "        print(f\"Validation Loss: {loss.item():.4f}\")\n",
    "\n",
    "        if wandb_:\n",
    "            run.log({\"Validation loss\": loss.item()})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"../models/arteries-2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Saving a CT volume tensor to image format (DICOM / NIFTI)\n",
    "#loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(10000))\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load(\"../models/arteries-2.pth\"))\n",
    "model.to('cuda')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Sample a batch of data from the test dataloader to make predictions\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Extract the image and segmentation from the batch\n",
    "image, seg = batch[\"image\"].to(device), batch[\"seg\"].to(device).squeeze()\n",
    "\n",
    "# Make predictions on the image\n",
    "output = model(image).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "#MIP\n",
    "#output,_ =torch.max(output, dim = -1)\n",
    "seg,_ = torch.max(seg, dim = -1)\n",
    "\n",
    "# Get the predicted segmentation class for each pixel\n",
    "pred_seg,_ = torch.max(output, dim = -1)\n",
    "\n",
    "\n",
    "# Visualize the image, ground truth segmentation, and predicted segmentation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "slice_idx = 16\n",
    "\n",
    "\n",
    "\n",
    "# Convert the image, ground truth segmentation, and predicted segmentation to NumPy arrays\n",
    "image = image.cpu().detach().numpy()\n",
    "seg = seg.cpu().detach().numpy()\n",
    "pred_seg = pred_seg.cpu().detach().numpy()\n",
    "\n",
    "# Get the first image, ground truth segmentation, and predicted segmentation from the batch\n",
    "image = image[0, 0, :, :, slice_idx]\n",
    "\n",
    "\n",
    "\n",
    "# Plot the image, ground truth segmentation, and predicted segmentation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(image, cmap=\"gray\")\n",
    "axes[0].set_title(\"Image\")\n",
    "axes[1].imshow(seg[0], cmap=\"gray\")\n",
    "axes[1].set_title(\"Ground Truth Segmentation\")\n",
    "axes[2].imshow(pred_seg[0], cmap=\"gray\")\n",
    "axes[2].set_title(\"Predicted Segmentation\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Extract the image and segmentation from the batch\n",
    "image, seg = batch[\"image\"], batch[\"seg\"].squeeze()\n",
    "mip,_ = torch.max(seg, dim = -1)\n",
    "\n",
    "image = image[0, 0, :, :, 48]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "colors = [(0, 0, 0, 0), (1, 0, 0, 1)]  # Start with transparent black, end with opaque red\n",
    "cmap_name = 'transparent_red'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors)\n",
    "ax.imshow(image, cmap=\"gray\")\n",
    "ax.imshow(mip[0], cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(pred_seg[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(seg[0].flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
