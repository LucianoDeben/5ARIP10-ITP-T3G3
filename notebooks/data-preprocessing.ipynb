{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation: Data Pre-Processing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from monai.data import DataLoader\n",
    "from monai.transforms import (EnsureChannelFirstd,\n",
    "Compose, LoadImaged, ResampleToMatchd, MapTransform)\n",
    "\n",
    "from monai.apps import TciaDataset\n",
    "from monai.apps.auto3dseg import AutoRunner\n",
    "from monai.bundle import ConfigParser\n",
    "\n",
    "from monai.config import print_config\n",
    "import json\n",
    "\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the collection and segmentation type\n",
    "collection, seg_type = \"HCC-TACE-Seg\", \"SEG\"\n",
    "\n",
    "# Create a dictionary to map the labels in the segmentation to the labels in the image\n",
    "label_dict = {'Liver': 0,\n",
    "  'Tumor': 1,\n",
    "  'vessels': 2,\n",
    "  'aorta': 3}\n",
    "\n",
    "class UndoOneHotEncoding(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        for key in self.keys:\n",
    "            data[key] = data[key].argmax(dim=1).unsqueeze(1)\n",
    "        return data\n",
    "    \n",
    "# Create a composed transform that loads the image and segmentation, resamples the image to match the segmentation,\n",
    "# and undoes the one-hot encoding of the segmentation\n",
    "transform = Compose(\n",
    "    [\n",
    "        LoadImaged(reader=\"PydicomReader\", keys=[\"image\", \"seg\"], label_dict=label_dict),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"seg\"]),\n",
    "        ResampleToMatchd(keys=\"image\", key_dst=\"seg\"),\n",
    "        UndoOneHotEncoding(keys=[\"seg\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a dataset for the training with a validation split\n",
    "train_dataset = TciaDataset(\n",
    "    root_dir=\"../data\",\n",
    "    collection=collection,\n",
    "    section=\"training\",\n",
    "    transform=transform,\n",
    "    download=True,\n",
    "    download_len=2,\n",
    "    seg_type=seg_type,\n",
    "    progress=True,\n",
    "    cache_rate=0.0,\n",
    "    val_frac=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a batch of data from the dataloader\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the batch data keys\n",
    "print(batch.keys())\n",
    "\n",
    "# Print the batch data shapes\n",
    "print(batch[\"image\"].shape, batch[\"seg\"].shape)\n",
    "\n",
    "# Print the batch data types\n",
    "print(batch[\"image\"].dtype, batch[\"seg\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the image and segmentation from the batch\n",
    "image, seg = batch[\"image\"], batch[\"seg\"]\n",
    "\n",
    "# Undo the one-hot encoding of the segmentation\n",
    "seg = seg.argmax(dim=1)\n",
    "seg = seg.unsqueeze(1)\n",
    "\n",
    "print(image.shape, seg.shape, seg.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample a slice from the image\n",
    "CT_slice = image[0, 0, :, :, 45]\n",
    "\n",
    "# Get the maximum segmentation class for each pixel in the slice\n",
    "_, CT_seg_slice_max = torch.max(seg[0, :, :, :, 45], dim=0)\n",
    "\n",
    "print(CT_slice.shape, CT_seg_slice_max.shape)\n",
    "\n",
    "# Plot the image and segmentation slice as a subplot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(CT_slice, cmap=\"gray\")\n",
    "axes[0].set_title(\"CT Image\")\n",
    "axes[1].imshow(CT_seg_slice_max, cmap=\"jet\")\n",
    "axes[1].set_title(\"CT Segmentation\")\n",
    "plt.colorbar(mappable=axes[1].imshow(CT_seg_slice_max, cmap='jet'), ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup AutoRunner for automatic segmentation model training and hyperparameter finetuning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a fold key to all the training data\n",
    "train_dataset.datalist = [{**item, 'fold': 0} for item in train_dataset.datalist]\n",
    "\n",
    "# Change \"seg\" to \"label\" in the datalist\n",
    "for item in train_dataset.datalist:\n",
    "    item[\"label\"] = item.pop(\"seg\")\n",
    "\n",
    "# Concatenate the training and test datalists\n",
    "data_list = {\"training\": train_dataset.datalist}\n",
    "\n",
    "datalist_file = \"../auto3dseg_datalist.json\"\n",
    "with open(datalist_file, \"w\") as f:\n",
    "    json.dump(data_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input configuration .yaml file\n",
    "input_config = {\n",
    "    \"name\": \"HCC-TACE-Seg\",\n",
    "    \"task\": \"segmentation\",  \n",
    "    \"modality\": \"CT\", \n",
    "    \"datalist\": \"../auto3dseg_datalist.json\", \n",
    "    \"dataroot\": \"../data\", \n",
    "}\n",
    "\n",
    "config_yaml = \"./auto3dseg_config.yaml\"\n",
    "ConfigParser.export_config_file(input_config, config_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = AutoRunner(work_dir = \"../data/auto3dseg\", input=input_config)\n",
    "runner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q2-5LSH0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
